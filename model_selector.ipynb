{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f045c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cfdd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bias_metrics_from_tensorboard(root_dir):\n",
    "    \"\"\"\n",
    "    Scans root_dir recursively, finds latest TensorBoard event file for each run,\n",
    "    loads scalar bias metrics, pivots them by step, and removes training loss and wet-day related tags.\n",
    "\n",
    "    Returns:\n",
    "        grouped_dfs: dict of {run_id: pd.DataFrame}, pivoted by step with cleaned tags\n",
    "    \"\"\"\n",
    "    latest_event_files = {}\n",
    "\n",
    "    # Step 1: Find latest event file for each run\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        event_files = [f for f in files if f.startswith(\"events.out.tfevents\")]\n",
    "        if not event_files:\n",
    "            continue\n",
    "\n",
    "        run_id = os.path.basename(root)\n",
    "        full_paths = [os.path.join(root, f) for f in event_files]\n",
    "        latest_file = max(full_paths, key=os.path.getmtime)\n",
    "        latest_event_files[run_id] = latest_file\n",
    "\n",
    "    # Step 2: Load scalars from each file\n",
    "    run_data = defaultdict(list)\n",
    "\n",
    "    for run_id, event_path in latest_event_files.items():\n",
    "        try:\n",
    "            ea = event_accumulator.EventAccumulator(event_path)\n",
    "            ea.Reload()\n",
    "            for tag in ea.Tags().get('scalars', []):\n",
    "                for s in ea.Scalars(tag):\n",
    "                    run_data[run_id].append((tag, s.step, s.value))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to load {event_path}: {e}\")\n",
    "\n",
    "    # Step 3: Convert to cleaned pivoted DataFrames\n",
    "    grouped_dfs = {}\n",
    "    drop_tags = {\n",
    "        'Loss/train',\n",
    "        'median_adjusted/Wet Days >1mm',\n",
    "        'median_adjusted/Very Wet Days >10mm',\n",
    "        'median_adjusted/Very Very Wet Days >20mm',\n",
    "        'median_adjusted/Dry Days'\n",
    "    }\n",
    "\n",
    "    for run_id, records in run_data.items():\n",
    "        df = pd.DataFrame(records, columns=[\"tag\", \"step\", \"value\"])\n",
    "        pivoted = df.pivot(index='step', columns='tag', values='value').sort_index()\n",
    "        pivoted = pivoted.drop(columns=[tag for tag in drop_tags if tag in pivoted.columns], errors='ignore')\n",
    "        grouped_dfs[run_id] = pivoted.dropna()\n",
    "\n",
    "    return grouped_dfs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9303d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"runs_LSTM/conus_gridmet_new/gfdl_esm4-gridmet\"\n",
    "grouped_dfs = load_bias_metrics_from_tensorboard(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89d05d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a0b8fd4c', 'ab2538cf', '08fb4524', '43705867', 'bfcdd469', 'd6a01914', '18e94be5', '2eba82c4', '4a89eced', '6aab0ccc', '759cef29', 'e91a39c1', '2bac29a2', '51cfede1', '92c02791', 'b3bdb62f', '73a5cbfa', '15950e27', '4f247c2c', 'fe95099e', 'b9905a36', '6dc6b33f', 'b2b3ea84', 'fe7cb7a4', 'e2ce595b']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"/pscratch/sd/k/kas7897/diffDownscale/jobs_revised_pca/access_cm2-gridmet\"\n",
    "second_level_dirs = []\n",
    "\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    # Only consider first-level subdirectories\n",
    "    if os.path.abspath(root) == os.path.abspath(base_dir):\n",
    "        for d in dirs:\n",
    "            subdir = os.path.join(root, d)\n",
    "            # List subdirectories inside each first-level subdirectory\n",
    "            for sub_root, sub_dirs, sub_files in os.walk(subdir):\n",
    "                if os.path.abspath(sub_root) == os.path.abspath(subdir):\n",
    "                    for sd in sub_dirs:\n",
    "                        sd = sd[:8]\n",
    "                        second_level_dirs.append(sd)\n",
    "        break  # Only need to process the top level\n",
    "\n",
    "print(second_level_dirs)\n",
    "\n",
    "grouped_dfs = {k: v for k, v in grouped_dfs.items() if any(sub in k for sub in second_level_dirs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85b43503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grouped_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1240bf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir1 = \"runs_revised/conus_pca/access_cm2-gridmet\"\n",
    "# grouped_dfs_pca = load_bias_metrics_from_tensorboard(root_dir1)\n",
    "\n",
    "# grouped_dfs = grouped_dfs | grouped_dfs_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "484ded37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_best_experiment_and_epoch(exp_dict, agg_method='median'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        exp_dict: dict of {exp_name: pd.DataFrame} with index=step, columns=indices (bias %)\n",
    "        agg_method: 'median', 'mean', or 'sum' to aggregate bias across indices\n",
    "    \n",
    "    Returns:\n",
    "        best_overall: (exp, step, score)\n",
    "        best_per_index: {index: (exp, step, bias)}\n",
    "        score_df: dataframe with all scores\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for exp, df in exp_dict.items():\n",
    "        for step, row in df.iterrows():\n",
    "            bias_vals = row.dropna()\n",
    "            if agg_method == 'median':\n",
    "                score = bias_vals.abs().median()\n",
    "            elif agg_method == 'mean':\n",
    "                score = bias_vals.abs().mean()\n",
    "            elif agg_method == 'sum':\n",
    "                score = bias_vals.abs().sum()\n",
    "            else:\n",
    "                raise ValueError(\"agg_method must be 'median', 'mean', or 'sum'\")\n",
    "\n",
    "            rows.append({\n",
    "                'exp': exp,\n",
    "                'step': step,\n",
    "                'score': score,\n",
    "                **row.to_dict()\n",
    "            })\n",
    "\n",
    "    score_df = pd.DataFrame(rows)\n",
    "\n",
    "    # Best overall (lowest aggregated score)\n",
    "    best_overall_row = score_df.loc[score_df['score'].idxmin()]\n",
    "    best_overall = (best_overall_row['exp'], best_overall_row['step'], best_overall_row['score'])\n",
    "\n",
    "    # Best for each index (closest to 0 bias)\n",
    "    indices = [col for col in score_df.columns if col not in ['exp', 'step', 'score']]\n",
    "    best_per_index = {}\n",
    "    for ind in indices:\n",
    "        best_row = score_df.loc[score_df[ind].abs().idxmin()]\n",
    "        best_per_index[ind] = (best_row['exp'], best_row['step'], best_row[ind])\n",
    "\n",
    "    return best_overall, best_per_index, score_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f15780c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_overall, best_per_index, scores = find_best_experiment_and_epoch(grouped_dfs, agg_method='median')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8175467e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('b9905a36_1979_2000_2001_2014', 200, 12.997745513916016)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6eec61bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss/validation': ('a466b5af_1979_2000_2001_2014', 400, 1.7437440156936646),\n",
       " 'median_adjusted/CDD (Yearly)': ('aba3352c_1979_2000_2001_2014',\n",
       "  30,\n",
       "  -0.10065137594938278),\n",
       " 'median_adjusted/CWD (Yearly)': ('71c3c134_1979_2000_2001_2014',\n",
       "  170,\n",
       "  2.6077096462249756),\n",
       " 'median_adjusted/R10mm': ('6197c63f_1979_2000_2001_2014',\n",
       "  30,\n",
       "  0.05280762165784836),\n",
       " 'median_adjusted/R20mm': ('22601d24_1979_2000_2001_2014',\n",
       "  0,\n",
       "  -4.671151161193848),\n",
       " 'median_adjusted/R95pTOT': ('d0594b3b_1979_2000_2001_2014',\n",
       "  20,\n",
       "  5.493357181549072),\n",
       " 'median_adjusted/R99pTOT': ('d0594b3b_1979_2000_2001_2014',\n",
       "  20,\n",
       "  5.493357181549072),\n",
       " 'median_adjusted/Rx1day': ('35bcd105_1979_2000_2001_2014',\n",
       "  0,\n",
       "  22.253765106201172),\n",
       " 'median_adjusted/Rx5day': ('35bcd105_1979_2000_2001_2014',\n",
       "  0,\n",
       "  36.42094039916992),\n",
       " 'median_adjusted/SDII (Monthly)': ('ef45008b_1979_2000_2001_2014',\n",
       "  0,\n",
       "  1.352627158164978)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_per_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "160c98c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Best Index Counts Per Experiment:\n",
      "a466b5af_1979_2000_2001_2014: 1 indices\n",
      "aba3352c_1979_2000_2001_2014: 1 indices\n",
      "71c3c134_1979_2000_2001_2014: 1 indices\n",
      "6197c63f_1979_2000_2001_2014: 1 indices\n",
      "22601d24_1979_2000_2001_2014: 1 indices\n",
      "d0594b3b_1979_2000_2001_2014: 2 indices\n",
      "35bcd105_1979_2000_2001_2014: 2 indices\n",
      "ef45008b_1979_2000_2001_2014: 1 indices\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_best_indices(best_per_index):\n",
    "    exp_counts = Counter()\n",
    "    for idx, (exp, step, bias) in best_per_index.items():\n",
    "        exp_counts[exp] += 1\n",
    "    return dict(exp_counts)\n",
    "\n",
    "counts = count_best_indices(best_per_index)\n",
    "print(\"üèÜ Best Index Counts Per Experiment:\")\n",
    "for exp, count in counts.items():\n",
    "    print(f\"{exp}: {count} indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f817587c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
