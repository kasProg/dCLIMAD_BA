{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f045c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cfdd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bias_metrics_from_tensorboard(root_dir):\n",
    "    \"\"\"\n",
    "    Scans root_dir recursively, finds latest TensorBoard event file for each run,\n",
    "    loads scalar bias metrics, pivots them by step, and removes training loss and wet-day related tags.\n",
    "\n",
    "    Returns:\n",
    "        grouped_dfs: dict of {run_id: pd.DataFrame}, pivoted by step with cleaned tags\n",
    "    \"\"\"\n",
    "    latest_event_files = {}\n",
    "\n",
    "    # Step 1: Find latest event file for each run\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        event_files = [f for f in files if f.startswith(\"events.out.tfevents\")]\n",
    "        if not event_files:\n",
    "            continue\n",
    "\n",
    "        run_id = os.path.basename(root)\n",
    "        full_paths = [os.path.join(root, f) for f in event_files]\n",
    "        latest_file = max(full_paths, key=os.path.getmtime)\n",
    "        latest_event_files[run_id] = latest_file\n",
    "\n",
    "    # Step 2: Load scalars from each file\n",
    "    run_data = defaultdict(list)\n",
    "\n",
    "    for run_id, event_path in latest_event_files.items():\n",
    "        try:\n",
    "            ea = event_accumulator.EventAccumulator(event_path)\n",
    "            ea.Reload()\n",
    "            for tag in ea.Tags().get('scalars', []):\n",
    "                for s in ea.Scalars(tag):\n",
    "                    run_data[run_id].append((tag, s.step, s.value))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to load {event_path}: {e}\")\n",
    "\n",
    "    # Step 3: Convert to cleaned pivoted DataFrames\n",
    "    grouped_dfs = {}\n",
    "    drop_tags = {\n",
    "        'Loss/train',\n",
    "        'median_adjusted/Wet Days >1mm',\n",
    "        'median_adjusted/Very Wet Days >10mm',\n",
    "        'median_adjusted/Very Very Wet Days >20mm',\n",
    "        'median_adjusted/Dry Days'\n",
    "    }\n",
    "\n",
    "    for run_id, records in run_data.items():\n",
    "        df = pd.DataFrame(records, columns=[\"tag\", \"step\", \"value\"])\n",
    "        pivoted = df.pivot(index='step', columns='tag', values='value').sort_index()\n",
    "        pivoted = pivoted.drop(columns=[tag for tag in drop_tags if tag in pivoted.columns], errors='ignore')\n",
    "        grouped_dfs[run_id] = pivoted.dropna()\n",
    "\n",
    "    return grouped_dfs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9303d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"runs_revised/conus_gridmet_cnn/access_cm2-gridmet\"\n",
    "grouped_dfs = load_bias_metrics_from_tensorboard(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d05d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a0b8fd4c', 'ab2538cf', '08fb4524', '43705867', 'bfcdd469', 'd6a01914', '18e94be5', '2eba82c4', '4a89eced', '6aab0ccc', '759cef29', 'e91a39c1', '2bac29a2', '51cfede1', '92c02791', 'b3bdb62f', '73a5cbfa', '15950e27', '4f247c2c', 'fe95099e', 'b9905a36', '6dc6b33f', 'b2b3ea84', 'fe7cb7a4', 'e2ce595b']\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# base_dir = \"/pscratch/sd/k/kas7897/diffDownscale/jobs_revised_pca/access_cm2-gridmet\"\n",
    "# second_level_dirs = []\n",
    "\n",
    "# for root, dirs, files in os.walk(base_dir):\n",
    "#     # Only consider first-level subdirectories\n",
    "#     if os.path.abspath(root) == os.path.abspath(base_dir):\n",
    "#         for d in dirs:\n",
    "#             subdir = os.path.join(root, d)\n",
    "#             # List subdirectories inside each first-level subdirectory\n",
    "#             for sub_root, sub_dirs, sub_files in os.walk(subdir):\n",
    "#                 if os.path.abspath(sub_root) == os.path.abspath(subdir):\n",
    "#                     for sd in sub_dirs:\n",
    "#                         sd = sd[:8]\n",
    "#                         second_level_dirs.append(sd)\n",
    "#         break  # Only need to process the top level\n",
    "\n",
    "# print(second_level_dirs)\n",
    "\n",
    "# grouped_dfs = {k: v for k, v in grouped_dfs.items() if any(sub in k for sub in second_level_dirs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b43503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grouped_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1240bf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir1 = \"runs_revised/conus_pca/access_cm2-gridmet\"\n",
    "# grouped_dfs_pca = load_bias_metrics_from_tensorboard(root_dir1)\n",
    "\n",
    "# grouped_dfs = grouped_dfs | grouped_dfs_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "484ded37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_best_experiment_and_epoch(exp_dict, agg_method='median'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        exp_dict: dict of {exp_name: pd.DataFrame} with index=step, columns=indices (bias %)\n",
    "        agg_method: 'median', 'mean', or 'sum' to aggregate bias across indices\n",
    "    \n",
    "    Returns:\n",
    "        best_overall: (exp, step, score)\n",
    "        best_per_index: {index: (exp, step, bias)}\n",
    "        score_df: dataframe with all scores\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for exp, df in exp_dict.items():\n",
    "        for step, row in df.iterrows():\n",
    "            bias_vals = row.dropna()\n",
    "            if agg_method == 'median':\n",
    "                score = bias_vals.abs().median()\n",
    "            elif agg_method == 'mean':\n",
    "                score = bias_vals.abs().mean()\n",
    "            elif agg_method == 'sum':\n",
    "                score = bias_vals.abs().sum()\n",
    "            else:\n",
    "                raise ValueError(\"agg_method must be 'median', 'mean', or 'sum'\")\n",
    "\n",
    "            rows.append({\n",
    "                'exp': exp,\n",
    "                'step': step,\n",
    "                'score': score,\n",
    "                **row.to_dict()\n",
    "            })\n",
    "\n",
    "    score_df = pd.DataFrame(rows)\n",
    "\n",
    "    # Best overall (lowest aggregated score)\n",
    "    best_overall_row = score_df.loc[score_df['score'].idxmin()]\n",
    "    best_overall = (best_overall_row['exp'], best_overall_row['step'], best_overall_row['score'])\n",
    "\n",
    "    # Best for each index (closest to 0 bias)\n",
    "    indices = [col for col in score_df.columns if col not in ['exp', 'step', 'score']]\n",
    "    best_per_index = {}\n",
    "    for ind in indices:\n",
    "        best_row = score_df.loc[score_df[ind].abs().idxmin()]\n",
    "        best_per_index[ind] = (best_row['exp'], best_row['step'], best_row[ind])\n",
    "\n",
    "    return best_overall, best_per_index, score_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f15780c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_overall, best_per_index, scores = find_best_experiment_and_epoch(grouped_dfs, agg_method='median')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8175467e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4de68857_1979_2000_2001_2014', 220, 7.9664692878723145)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eec61bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss/validation': ('40970740_1979_2000_2001_2014', 380, 2.3028457164764404),\n",
       " 'median_adjusted/CDD (Yearly)': ('74cc5d77_1979_2000_2001_2014',\n",
       "  30,\n",
       "  0.18543754518032074),\n",
       " 'median_adjusted/CWD (Yearly)': ('cd2c368c_1979_2000_2001_2014',\n",
       "  80,\n",
       "  -0.0371057502925396),\n",
       " 'median_adjusted/R10mm': ('cd2c368c_1979_2000_2001_2014',\n",
       "  70,\n",
       "  -0.5277726054191589),\n",
       " 'median_adjusted/R20mm': ('6816184f_1979_2000_2001_2014', 130, 0.0),\n",
       " 'median_adjusted/R95pTOT': ('74cc5d77_1979_2000_2001_2014',\n",
       "  250,\n",
       "  -0.13641822338104248),\n",
       " 'median_adjusted/R99pTOT': ('74cc5d77_1979_2000_2001_2014',\n",
       "  250,\n",
       "  -0.13641822338104248),\n",
       " 'median_adjusted/Rx1day': ('cd2c368c_1979_2000_2001_2014',\n",
       "  70,\n",
       "  5.9045257568359375),\n",
       " 'median_adjusted/Rx5day': ('40970740_1979_2000_2001_2014',\n",
       "  0,\n",
       "  3.2955329418182373),\n",
       " 'median_adjusted/SDII (Monthly)': ('4de68857_1979_2000_2001_2014',\n",
       "  60,\n",
       "  0.8129074573516846)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_per_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "160c98c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Best Index Counts Per Experiment:\n",
      "40970740_1979_2000_2001_2014: 2 indices\n",
      "74cc5d77_1979_2000_2001_2014: 3 indices\n",
      "cd2c368c_1979_2000_2001_2014: 3 indices\n",
      "6816184f_1979_2000_2001_2014: 1 indices\n",
      "4de68857_1979_2000_2001_2014: 1 indices\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_best_indices(best_per_index):\n",
    "    exp_counts = Counter()\n",
    "    for idx, (exp, step, bias) in best_per_index.items():\n",
    "        exp_counts[exp] += 1\n",
    "    return dict(exp_counts)\n",
    "\n",
    "counts = count_best_indices(best_per_index)\n",
    "print(\"üèÜ Best Index Counts Per Experiment:\")\n",
    "for exp, count in counts.items():\n",
    "    print(f\"{exp}: {count} indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f817587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_experiment_with_stability(exp_dict, agg_method='median', \n",
    "                                       stability_window=10, min_epochs=50,\n",
    "                                       loss_weight=0.3, bias_weight=0.7):\n",
    "    \"\"\"\n",
    "    Find best experiment considering both bias performance AND training stability.\n",
    "    \n",
    "    Args:\n",
    "        stability_window: number of recent epochs to check for stability\n",
    "        min_epochs: minimum training epochs before considering a model\n",
    "        loss_weight, bias_weight: relative importance of loss vs bias (should sum to 1)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for exp, df in exp_dict.items():\n",
    "        if len(df) < min_epochs:\n",
    "            continue\n",
    "            \n",
    "        # Get loss data (you'll need to load this separately)\n",
    "        loss_data = load_loss_data(exp)  # You'll need to implement this\n",
    "        \n",
    "        for step in df.index[min_epochs:]:  # Only consider after min_epochs\n",
    "            row = df.loc[step]\n",
    "            bias_vals = row.dropna()\n",
    "            \n",
    "            if len(bias_vals) == 0:\n",
    "                continue\n",
    "                \n",
    "            # 1. Calculate bias score\n",
    "            if agg_method == 'median':\n",
    "                bias_score = bias_vals.abs().median()\n",
    "            elif agg_method == 'mean':\n",
    "                bias_score = bias_vals.abs().mean()\n",
    "            else:\n",
    "                bias_score = bias_vals.abs().sum()\n",
    "            \n",
    "            # 2. Check training stability\n",
    "            stability_metrics = calculate_stability(loss_data, step, stability_window)\n",
    "            \n",
    "            # 3. Combined score\n",
    "            combined_score = (loss_weight * stability_metrics['loss_score'] + \n",
    "                            bias_weight * bias_score)\n",
    "            \n",
    "            results.append({\n",
    "                'exp': exp,\n",
    "                'step': step,\n",
    "                'bias_score': bias_score,\n",
    "                'loss_trend': stability_metrics['loss_trend'],\n",
    "                'loss_variance': stability_metrics['loss_variance'],\n",
    "                'is_stable': stability_metrics['is_stable'],\n",
    "                'epochs_since_improvement': stability_metrics['epochs_since_improvement'],\n",
    "                'combined_score': combined_score,\n",
    "                **row.to_dict()\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Filter to only stable models\n",
    "    stable_results = results_df[results_df['is_stable']].copy()\n",
    "    \n",
    "    if len(stable_results) == 0:\n",
    "        print(\"‚ö†Ô∏è No stable models found! Relaxing stability criteria...\")\n",
    "        stable_results = results_df\n",
    "    \n",
    "    # Best stable model\n",
    "    best_stable = stable_results.loc[stable_results['combined_score'].idxmin()]\n",
    "    \n",
    "    return best_stable, stable_results\n",
    "\n",
    "def calculate_stability(loss_data, current_step, window=10):\n",
    "    \"\"\"Calculate training stability metrics\"\"\"\n",
    "    if current_step < window:\n",
    "        return {'is_stable': False, 'loss_trend': float('inf'), \n",
    "                'loss_variance': float('inf'), 'loss_score': 1.0,\n",
    "                'epochs_since_improvement': 0}\n",
    "    \n",
    "    recent_loss = loss_data[current_step-window:current_step+1]\n",
    "    \n",
    "    # 1. Loss trend (should be flat or slightly decreasing)\n",
    "    loss_trend = np.polyfit(range(len(recent_loss)), recent_loss, 1)[0]\n",
    "    \n",
    "    # 2. Loss variance (should be low)\n",
    "    loss_variance = np.var(recent_loss)\n",
    "    \n",
    "    # 3. Epochs since last significant improvement\n",
    "    best_loss_idx = np.argmin(loss_data[:current_step+1])\n",
    "    epochs_since_improvement = current_step - best_loss_idx\n",
    "    \n",
    "    # 4. Stability criteria\n",
    "    is_stable = (\n",
    "        abs(loss_trend) < 0.001 and  # Trend is nearly flat\n",
    "        loss_variance < 0.01 and     # Low variance\n",
    "        epochs_since_improvement < window * 2  # Recent improvement\n",
    "    )\n",
    "    \n",
    "    # 5. Loss score (normalized, lower is better)\n",
    "    loss_score = min(recent_loss) / max(loss_data)  # Relative to worst loss\n",
    "    \n",
    "    return {\n",
    "        'is_stable': is_stable,\n",
    "        'loss_trend': loss_trend,\n",
    "        'loss_variance': loss_variance,\n",
    "        'loss_score': loss_score,\n",
    "        'epochs_since_improvement': epochs_since_improvement\n",
    "    }\n",
    "\n",
    "def load_loss_data(exp_name):\n",
    "    \"\"\"Load training loss for a specific experiment\"\"\"\n",
    "    # You'll need to implement this based on your TensorBoard data\n",
    "    # Return array of loss values by epoch\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
